{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctsi-git/VQGAN-Clip/blob/main/AI_Generated_Images_VQGAN_%2B_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clJsMT0Eqizk"
      },
      "source": [
        "# Create Realistic AI-Generated Images With VQGAN + CLIP\n",
        "\n",
        "by [Max Woolf](https://minimaxir.com). \n",
        "\n",
        "This notebook allows you to create realistic AI generated images with as few clicks as possible for free! No coding or machine learning knowledge required!\n",
        "\n",
        "This notebook is forked with significant usability and technical optimizations from the [original Colab notebook](https://colab.research.google.com/drive/1Foi0mCSE6NrW9oI3Fhni7158Krz4ZXdH) by [@ak92501](https://twitter.com/ak92501) which includes an implementation of VQGAN + CLIP w/ Pooling. The Notebook was originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). The original BigGAN+CLIP method was by https://twitter.com/advadnoun. Added some explanations and modifications by Eleiber#8347, pooling trick by Crimeacs#8222 (https://twitter.com/EarthML1). For more elaborate customization, see the original notebook or [Zoetrope 5](https://colab.research.google.com/drive/1LpEbICv1mmta7Qqic1IcRTsRsq7UKRHM) by [@classpectanon](https://twitter.com/classpectanon).\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Run the cells below by clicking the **Play** button on the left of the cell (also visible when mousing-over the cell)\n",
        "\n",
        "_Last Updated: Aug 22th 2021_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUfzT60ZZ9q",
        "outputId": "1f17461c-d817-4912-d4ce-4d6a902c7136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU\n",
        "#@markdown Run this cell to see what GPU the Colab Notebook is running. Ideally, it's not a K80 which is the slowest one.\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "VA1PHoJrRiK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df869a52-15b7-4d98-a237-c448f7399c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 195 (delta 12), reused 22 (delta 9), pack-reused 168\u001b[K\n",
            "Receiving objects: 100% (195/195), 8.91 MiB | 29.71 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1335, done.\u001b[K\n",
            "remote: Counting objects: 100% (525/525), done.\u001b[K\n",
            "remote: Compressing objects: 100% (493/493), done.\u001b[K\n",
            "remote: Total 1335 (delta 58), reused 481 (delta 30), pack-reused 810\u001b[K\n",
            "Receiving objects: 100% (1335/1335), 412.35 MiB | 39.89 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n",
            "Cloning into 'icon-image'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 84 (delta 45), reused 58 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), done.\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting icon_font_to_png\n",
            "  Downloading icon_font_to_png-0.4.1-py2.py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.12.5 in /usr/local/lib/python3.7/dist-packages (from icon_font_to_png) (2.23.0)\n",
            "Collecting tinycss>=0.4\n",
            "  Downloading tinycss-0.4.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.12.5->icon_font_to_png) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.12.5->icon_font_to_png) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.12.5->icon_font_to_png) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.12.5->icon_font_to_png) (1.24.3)\n",
            "Building wheels for collected packages: fire, tinycss\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=7c1992df3e4c18050fa72e6bbb837eaf5f99edbceef1e6e663d310f0112309b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for tinycss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinycss: filename=tinycss-0.4-py3-none-any.whl size=43955 sha256=e5b1665fb54c09110d6d26a63e386812336f0ade5434929e2e3dbbfb97c7e545\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/66/e8/e53d7a476011891fa51a5ee83a2d1852b19b258f975055429b\n",
            "Successfully built fire tinycss\n",
            "Installing collected packages: tinycss, icon-font-to-png, fire\n",
            "Successfully installed fire-0.4.0 icon-font-to-png-0.4.1 tinycss-0.4\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 42.7 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.43.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 58.9 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.11)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, future\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=97900ea51a90e02910bd6fb5e69478f93cba6fc683fb6ceb59a6191fe66bad8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=fd09efc76de8c7e1cc4f55585bf5135b9006154fe75295846bfaac9695ba5033\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built antlr4-python3-runtime future\n",
            "Installing collected packages: setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, future, antlr4-python3-runtime, pytorch-lightning, omegaconf, ftfy\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.1.0 ftfy-6.1.1 future-0.18.2 multidict-6.0.2 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 setuptools-59.5.0 torchmetrics-0.7.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.6.3-py2.py3-none-any.whl (474 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 474 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.7)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.3\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.5-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.5\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement imagio (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for imagio\u001b[0m\n",
            "Downloading ImageNet 16384\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100  934M  100  934M    0     0  14.5M      0  0:01:04  0:01:04 --:--:-- 14.9M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   692  100   692    0     0   1207      0 --:--:-- --:--:-- --:--:--  1207\n"
          ]
        }
      ],
      "source": [
        "#@title Download Models and Install/Load Packages (may take a few minutes)\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers.git\n",
        "!git clone https://github.com/minimaxir/icon-image.git\n",
        "!pip install Pillow numpy fire icon_font_to_png\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install imageio-ffmpeg   \n",
        "!pip install einops\n",
        "!pip install imagio          \n",
        "!mkdir steps\n",
        "\n",
        "print(\"Downloading ImageNet 16384\")\n",
        "\n",
        "!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1'\n",
        "!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1'\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.insert(1, '/content/taming-transformers')\n",
        "sys.path.insert(1, '/content/icon-image')\n",
        "\n",
        "from icon_image import gen_icon\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import taming.modules \n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm.notebook import tqdm\n",
        "from shutil import move\n",
        "import os\n",
        "\n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.augs = nn.Sequential(\n",
        "            # K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomVerticalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            # K.RandomSharpness(0.3,p=0.4),\n",
        "            # K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5),\n",
        "            # K.RandomCrop(size=(self.cut_size,self.cut_size), p=0.5),\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
        "            K.RandomPerspective(0.7,p=0.7),\n",
        "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
        "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n",
        "            \n",
        ")\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        \n",
        "        for _ in range(self.cutn):\n",
        "\n",
        "            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            # offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            # offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            # cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "            # cutout = transforms.Resize(size=(self.cut_size, self.cut_size))(input)\n",
        "            \n",
        "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
        "            cutouts.append(cutout)\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tthw0YaispD"
      },
      "source": [
        "## Icon Background (Optional)\n",
        "\n",
        "A surprisingly effective trick to improve the generation quality of images if you have a specific outcome in mind to generate an icon to serve an initial image to start generation and/or an image to target during generation. You can select any of the [free Font Awesome icons](https://fontawesome.com/v5.15/icons?d=gallery&p=2&m=free) to use. Just click on an icon you want to get the `icon_name` such as `fas fa-robot`, then use that with next cell will generate an icon image to help steer the AI image generation.\n",
        "\n",
        "See [this GitHub repository](https://github.com/minimaxir/icon-image) for more information on configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qxrUUDzpshPn"
      },
      "outputs": [],
      "source": [
        "icon_name = \"fas fa-biohazard\" #@param {type:\"string\"}\n",
        "bg_width = 600 #@param {type:\"integer\"}\n",
        "bg_height = 600 #@param {type:\"integer\"}\n",
        "icon_size = 500 #@param {type:\"integer\"}\n",
        "icon_color = \"black\" #@param {type:\"string\"}\n",
        "bg_color = \"white\" #@param {type:\"string\"}\n",
        "icon_opacity = 0.6 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "bg_noise_opacity = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "align = \"center\" #@param [\"center\", \"left\", \"right\", \"top\", \"bottom\"]\n",
        "\n",
        "icon_config = {\n",
        "    \"icon_name\": icon_name,\n",
        "    \"bg_width\": bg_width,\n",
        "    \"bg_height\": bg_height,\n",
        "    \"icon_size\": icon_size,\n",
        "    \"icon_color\": icon_color,\n",
        "    \"bg_color\": bg_color,\n",
        "    \"icon_opacity\": icon_opacity,\n",
        "    \"bg_noise_opacity\": bg_noise_opacity,\n",
        "    \"align\": align,\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "try:\n",
        "  for filename in ['fa-brands-400.ttf', 'fa-regular-400.ttf', 'fa-solid-900.ttf', 'fontawesome.min.css']:\n",
        "      move(os.path.join(\"/content\", 'icon-image', filename), os.path.join(\"/content\", filename))\n",
        "except FileNotFoundError:\n",
        "  pass\n",
        "\n",
        "gen_icon(**icon_config)\n",
        "display.display(display.Image('icon.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0qN8T1EzPn7"
      },
      "source": [
        "## AI Image Generation Settings\n",
        "\n",
        "The following cell allows you to set the training parameters for image generation:\n",
        "\n",
        "### Generation Settings\n",
        "\n",
        "- `texts`: The text prompt(s) you want the AI to generate an image from.\n",
        "  - You can include multiple prompts by separating them with a `|`, and the AI will attempt to optimize for all prompts simultaneously, e.g. `apple | painting of a calm sunset`\n",
        "  - You can apply a weight to each prompt by appending a `:{weight}` to each prompt, and the AI will attempt to favor prompts with a higher weight proportionally more, e.g. `apple:3 | painting of a calm sunset`\n",
        "  - You can apply a negative weight to get the *opposite* of what the text is, which can result in chaos. (in the case of `a portrait of Elon Musk:3 | 3d rendering in unreal engine:-1`, what is the opposite of a 3d rendering? Only one way to find out!)\n",
        "\n",
        "- `width`, `height`: Width and height of the image in pixels. Smaller images generate faster but are less detailed.\n",
        "  - Going too high above the default 600x600px size may result in the GPU going out-of-memory.\n",
        "  - For 4:3 images, I recommend 640x480; for 16:9 images, I recommend 640x360.\n",
        "\n",
        "- `init_image`: The initial image filename for starting the generation and finetuning. You can upload an image by opening the Colab Notebook sidebar, clicking the Folder icon, and uploading an image to the top level.\n",
        "  - If not specified, generation will start with a solid color.\n",
        "  - The image will be resized to the specified width/height.\n",
        "  - `init_image_icon` will use the icon specified in the previous cell as the `init_image`.\n",
        "\n",
        "- `target_images`: The target image filename(s) for the generation to target. \n",
        "  - You can use multiple images as noted in the `texts` section. It's strongly recommended to tweak weights of both text prompts and image prompts if doing so.\n",
        "  - `target_image_icon` will use the icon specified in the previous cell as the `target_image`.\n",
        "\n",
        "### Training Settings\n",
        "\n",
        "- `learning_rate`: Learning rate for the model which controls the speed in which the model optimizes for the prompts. If too high, model can diverge; if too low, model may not train.\n",
        "  - ~0.2 is recommend if training without an `init_image`; ~0.1 is recommended if using one.\n",
        "\n",
        "- `max_steps`: Number of steps for training the model; the more steps, the better the generation.\n",
        "\n",
        "- `images_interval`: Number of steps for the training to check in and output an image of what is trained so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pf8a78a2WKoU"
      },
      "outputs": [],
      "source": [
        "# Fixed parameters\n",
        "icon_path = \"icon.png\"\n",
        "model_name = \"vqgan_imagenet_f16_16384\"\n",
        "seed = 42\n",
        "\n",
        "texts = 'A beautiful nude female robot standing over a pile of human skulls' #@param {type:\"string\"}\n",
        "width = 600 #@param {type:\"integer\"}\n",
        "height = 600 #@param {type:\"integer\"}\n",
        "init_image = \"\" #@param {type:\"string\"}\n",
        "init_image_icon = False #@param {type:\"boolean\"}\n",
        "if init_image_icon:\n",
        "  assert os.path.exists(icon_path), \"No icon has been generated from the previous cell\"\n",
        "  init_image = icon_path\n",
        "\n",
        "target_images = \"\" #@param {type:\"string\"}\n",
        "target_image_icon = False #@param {type:\"boolean\"}\n",
        "if target_image_icon:\n",
        "  assert os.path.exists(icon_path), \"No icon has been generated from the previous cell\"\n",
        "  target_images = icon_path\n",
        "\n",
        "#@markdown ---\n",
        "learning_rate = 0.2 #@param {type:\"slider\", min:0.00, max:0.30, step:0.01}\n",
        "max_steps = 300 #@param {type:\"integer\"}\n",
        "images_interval = 20 #@param {type:\"integer\"}\n",
        "\n",
        "gen_config = {\n",
        "    \"texts\": texts,\n",
        "    \"width\": width,\n",
        "    \"height\": height,\n",
        "    \"init_image\": \"<icon>\" if init_image_icon else init_image,\n",
        "    \"target_images\": \"<icon>\" if target_image_icon else target_images,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"training_seed\": 42,\n",
        "    \"model\": \"vqgan_imagenet_f16_16384\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZdlpRFL8UAlW"
      },
      "outputs": [],
      "source": [
        "#@title Start AI Image Generation!\n",
        "\n",
        "!rm -rf steps\n",
        "!mkdir steps\n",
        "\n",
        "metadata = PngInfo()\n",
        "for k, v in gen_config.items():\n",
        "    try:\n",
        "        metadata.add_text(\"AI_ \" + k, str(v))\n",
        "    except UnicodeEncodeError:\n",
        "        pass\n",
        "\n",
        "if init_image_icon or target_image_icon:\n",
        "  for k, v in icon_config.items():\n",
        "    try:\n",
        "        metadata.add_text(\"AI_Icon_ \" + k, str(v))\n",
        "    except UnicodeEncodeError:\n",
        "        pass\n",
        "\n",
        "model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", 'vqgan_openimages_f16_8192':'OpenImages 8912',\n",
        "                \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\n",
        "name_model = model_names[model_name]     \n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "if init_image == \"None\":\n",
        "    init_image = None\n",
        "if target_images == \"None\" or not target_images:\n",
        "    model_target_images = []\n",
        "else:\n",
        "    model_target_images = target_images.split(\"|\")\n",
        "    model_target_images = [image.strip() for image in model_target_images]\n",
        "\n",
        "model_texts = [phrase.strip() for phrase in texts.split(\"|\")]\n",
        "if model_texts == ['']:\n",
        "    model_texts = []\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompts=model_texts,\n",
        "    image_prompts=model_target_images,\n",
        "    noise_prompt_seeds=[],\n",
        "    noise_prompt_weights=[],\n",
        "    size=[width, height],\n",
        "    init_image=init_image,\n",
        "    init_weight=0.,\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{model_name}.yaml',\n",
        "    vqgan_checkpoint=f'{model_name}.ckpt',\n",
        "    step_size=learning_rate,\n",
        "    cutn=32,\n",
        "    cut_pow=1.,\n",
        "    display_freq=images_interval,\n",
        "    seed=seed,\n",
        ")\n",
        "from urllib.request import urlopen\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if model_texts:\n",
        "    print('Using texts:', model_texts)\n",
        "if model_target_images:\n",
        "    print('Using image prompts:', model_target_images)\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "# clock=deepcopy(perceptor.visual.positional_embedding.data)\n",
        "# perceptor.visual.positional_embedding.data = clock/clock.max()\n",
        "# perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "    e_dim = 256\n",
        "    n_toks = model.quantize.n_embed\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "# z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "# z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "# normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                            std=[0.229, 0.224, 0.225])\n",
        "\n",
        "if args.init_image:\n",
        "    if 'http' in args.init_image:\n",
        "        img = Image.open(urlopen(args.init_image))\n",
        "    else:\n",
        "        img = Image.open(args.init_image)\n",
        "    pil_image = img.convert('RGB')\n",
        "    if pil_image.size != (width, height):\n",
        "      print(f\"Resizing source image to {width}x{height}\")\n",
        "      pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "    pil_tensor = TF.to_tensor(pil_image)\n",
        "    z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n",
        "else:\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    # z = one_hot @ model.quantize.embedding.weight\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
        "    z = torch.rand_like(z)*2\n",
        "z_orig = z.clone()\n",
        "z.requires_grad_(True)\n",
        "opt = optim.Adam([z], lr=args.step_size)\n",
        "scheduler = StepLR(opt, step_size=5, gamma=0.95)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "\n",
        "\n",
        "pMs = []\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "for prompt in args.image_prompts:\n",
        "    path, weight, stop = parse_prompt(prompt)\n",
        "    img = Image.open(path)\n",
        "    pil_image = img.convert('RGB')\n",
        "    img = resize_image(pil_image, (sideX, sideY))\n",
        "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "    embed = perceptor.encode_image(normalize(batch)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "def synth(z):\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "    else:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z)\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png', pnginfo=metadata)\n",
        "    display.display(display.Image('progress.png'))\n",
        "\n",
        "def ascend_txt():\n",
        "    # global i\n",
        "    out = synth(z)\n",
        "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "    \n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*args.init_weight) / 2)\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = Image.fromarray(img)\n",
        "    # imageio.imwrite(f'./steps/{i:03d}.png', np.array(img))\n",
        "\n",
        "    img.save(f\"./steps/{i:03d}.png\", pnginfo=metadata)\n",
        "    return result\n",
        "\n",
        "def train(i):\n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    \n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    scheduler.step()\n",
        "    with torch.no_grad():\n",
        "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "try:\n",
        "    for i in tqdm(range(max_steps)):\n",
        "          train(i)\n",
        "    checkin(max_steps, ascend_txt())\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bZ-Lt8o3R2D"
      },
      "source": [
        "You can right-click the final image and Save As to save it locally, Copy and Paste it directly to another application/social media site, or go into the `/steps/` folder to view all genenerate images for each step.\n",
        "\n",
        "If you do use these images, please note that they were created by VQGAN + CLIP and/or provide a link to this Notebook so others can make their own images too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq3tv3VUwkA3"
      },
      "source": [
        "## Generate Video\n",
        "\n",
        "You can generate and download a video of the AI generation you just did by running the following cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEkgIdoWwr8Z"
      },
      "outputs": [],
      "source": [
        "frame_rate = 30 #@param {type:\"number\"}\n",
        "\n",
        "print(\"Rendering Video...\")\n",
        "result = os.system(f\"ffmpeg -y -r {frame_rate} -i /content/steps/%03d.png -c:v libx264 -vf fps={frame_rate} -pix_fmt yuv420p /content/vqgan_clip.mp4\")\n",
        "print(\"Video saved at vqgan_clip.mp4!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/vqgan_clip.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6oveFmHKdh4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7LwGXVwxoS9"
      },
      "source": [
        "## Notes / Helpful Tips\n",
        "\n",
        "- You can further constrain image generation to follow an icon shape, which can have [incredible](https://twitter.com/minimaxir/status/1423836227409567747) [results](https://twitter.com/minimaxir/status/1423800629248479237). The trick:\n",
        "  - Use an icon image for **both** `init_image` and `target_images`, ideally with an `icon_opacity` less than 1.0.\n",
        "  - Apply a very high weight to the `texts` prompt, e.g. `reality is an illusion:8`. Decrease the weight iteratively to allow the generation to follow the icon shape better.\n",
        "- This notebook forces the use of the ImageNet 16384 VQGAN as that generates the best images for the vast majority of use cases (exceptions are images with sharp shapes, such as text, pixel art, and anime). If more research into alternate VQGANs continues, then a selector may be added.\n",
        "- The training uses a slight learning rate decay (multiply LR by 95% every 5 steps) to avoid destablization when further in the training.\n",
        "- The config parameters are stored as PNG tEXt metadata within each generated image, so you can retrieve the configuration used for each generated image if necessary using a tool like `exiftool` or https://exif.tools/. This metadata is stripped when an image is uploaded to social media."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdOwQoYvmaKl"
      },
      "source": [
        "## License\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2021 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "---\n",
        "\n",
        "Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "AI-Generated Images VQGAN + CLIP",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}